{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab06.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 6: Linear Regression\n",
    "\n",
    "In this lab, you will review the details of linear regression. In particular:\n",
    "\n",
    "* How to formulate Matrices and solutions to Ordinary Least Squares (OLS).\n",
    "* `sns.lmplot` as a quick visual for Simple Linear Regression (SLR).\n",
    "* `scikit-learn`, or `sklearn` for short, a real-world data science tool that is more robust and flexible than analytical or `scipy.optimize` solutions. \n",
    "\n",
    "You will also practice interpreting residual plots (vs. fitted values) and the Multiple $R^2$ metric used in Multiple Linear Regression.\n",
    "\n",
    "**The on-time deadline is Tuesday, October 10th, 11:59 PM. Please read the syllabus for the grace period policy. No late submissions beyond the grace period will be accepted.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk to others about the labs, we ask that you **write your solutions individually**. If you do discuss the assignments with others, please **include their names** in the following cell:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *List names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Walk-Through\n",
    "In addition to the lab notebook, we have also released a prerecorded walk-through video of the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video.\n",
    "\n",
    "**Note:** This video is recorded in Spring 2022. There may be slight inconsistencies between the version you are viewing and the version used in the recording, but content is identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"IkkhAr3e19Q\", list = 'PLQCcNQgUcDfpuwnASdUyvQky51ZcYMWSy', listType = 'playlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Data Loading\n",
    "\n",
    "For the first part of this lab, you will predict fuel efficiency (`mpg`) of several models of automobiles using a **single feature**: engine power (`horsepower`). For the second part, you will perform feature engineering on **multiple features** to better predict fuel efficiency.\n",
    "\n",
    "First, let's load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we load the fuel dataset, and drop any rows that have missing data.\n",
    "vehicle_data = sns.load_dataset('mpg').dropna()\n",
    "vehicle_data = vehicle_data.sort_values('horsepower', ascending=True)\n",
    "vehicle_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 392 datapoints and 8 potential features (plus our observed $y$ values, `mpg`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to fit a line to the plot below, which shows `mpg` vs. `horsepower` for several models of automobiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to visualize the data. \n",
    "sns.scatterplot(data=vehicle_data, x='horsepower', y='mpg');\n",
    "plt.title(\"mpg vs horsepower\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 1: Ordinary Least Squares\n",
    "Recall that the equation for Simple Linear Regression (SLR) has two $\\theta$ coefficients: $\\theta_0$ and $\\theta_1$, and is written as follows:\n",
    "\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x$$\n",
    "\n",
    "\n",
    "If we have many pairs of $(x_i, y_i)$ values, $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$, we would need to apply the SLR equation to each pair $n$ times. This is quite repetitive, so let's re-formulate our SLR equation using linear algebra. We'll:\n",
    "\n",
    "* Rewrite our $\\theta$ coefficients as a vector $\\theta = [\\theta_0, \\theta_1]$.\n",
    "* Stack our $x_i$ values into a vector $\\vec{x}$ with $n$ values.\n",
    "* Stack our $y_i$ values into a vector $\\mathbb{Y}$ of all $n$ observations in our sample.\n",
    "\n",
    "Then our prediction vector $\\hat{\\mathbb{Y}}$ can then be written as:\n",
    "$$\\Large \\hat{\\mathbb{Y}} = {\\theta_0} \\vec{1}_n + {\\theta_1} \\vec{x} = \\begin{bmatrix} | & | \\\\ \\vec{1}_n & \\vec{x} \\\\ | & | \\end{bmatrix} \\begin{bmatrix} {\\theta_0} \\\\ {\\theta_1} \\end{bmatrix} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2 \\\\ \\vdots & \\vdots \\\\ 1 & x_n \\end{bmatrix} \\begin{bmatrix} {\\theta_0} \\\\ {\\theta_1} \\end{bmatrix} = \\Bbb{X} \\begin{bmatrix} {\\theta_0} \\\\ {\\theta_1} \\end{bmatrix} = \\mathbb{X} \\theta$$\n",
    "\n",
    "where $\\mathbb{X} \\in \\mathbb{R}^{n\\times2}$ is the **design matrix** with a **bias** column of all ones to account for the intercept, $\\theta_0$, and one **feature** for all $n$ datapoints in our sample. \n",
    "\n",
    "Our equation now matches the Ordinary Least Squares (OLS) equation! \n",
    "$$\\Large \\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$$\n",
    "\n",
    "\n",
    "### Expanding OLS to Multiple Linear Regression\n",
    "The OLS equation can be expanded to cases when we have more than one feature, like in the case of our Multiple Linear Regression (MLR) model, where we can have $p$ features: \n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_1 + \\dots + \\theta_p x_p$$\n",
    "\n",
    "Our OLS equation can be generalized as  \n",
    "$$\\Large \\hat{\\mathbb{Y}} = \\begin{bmatrix} \n",
    "    1  & x_{1,1}  & x_{1,2}  & \\cdots & x_{1,p}\\\\\n",
    "    1  & x_{2,1}  & x_{2,2}    & \\cdots & x_{2,p}\\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "    1  & x_{n,1}  & x_{n,2}    & \\cdots & x_{n,p}\n",
    "\\end{bmatrix}  \n",
    "\\begin{bmatrix} {\\theta_0} \\\\ {\\theta_1} \\\\ \\vdots \\\\ \\theta_{p} \\end{bmatrix} = \\mathbb{X} \\theta\n",
    "$$\n",
    "with a\n",
    "* **prediction vector** $\\mathbb{Y} \\in \\mathbb{R}^{n}$,\n",
    "* **design matrix** $\\mathbb{X} \\in \\mathbb{R}^{n\\times(p + 1)}$ representing the $p$ features for all $n$ datapoints in our sample,\n",
    "* and a **parameter vector** $\\theta \\in \\mathbb{R}^{p + 1}$.\n",
    "\n",
    "Simple linear regression is a special case of OLS when $p=1$.\n",
    "\n",
    "### Today's Lab\n",
    "\n",
    "In today's lab, we'll explore the OLS equations with different examples.\n",
    "* In Question 1, we'll write code to evaluate linear algebra and apply those functions to predict `mpg` from one feature, `horsepower`. Since we only have one feature ($p=1$), this is a case of simple linear regression.\n",
    "* In Question 2, we'll explore how transforming data affects our prediction by using horsepower squared (`hp^2`) as our feature instead of `horsepower`. \n",
    "* In Question 3, we'll combine the features from questions 1 and 2 and use *multiple linear regression* on 2 features: `horsepower` and `hp^2`.\n",
    "* Finally for question 4, we'll explore how redundant features affect our calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1a: Construct $\\mathbb{X}$ with an intercept term\n",
    "The OLS equation is displayed for your reference: \n",
    "$$\\Large \\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$$\n",
    "\n",
    "Because we have an intercept term $\\theta_0$ in our parameter vector $\\theta$, our design matrix $\\mathbb{X}$ needs a column with all-ones such that the resulting matrix expression, $\\hat{\\mathbb{Y}} = \\mathbb{X} \\theta$, represents $n$ linear equations, where equation $i$ is \n",
    "$$\\hat{y_i} = \\theta_0 \\cdot 1 + \\theta_1 \\cdot x_{i, 1} + \\dots + \\theta_p \\cdot x_{i, p}$$ \n",
    "where $x_{i, j}$ is the $j^{th}$ feature of the $i^{th}$ datapoint. The constant all-ones column of $\\mathbb{X}$ is sometimes called the bias feature; $\\theta_0$ is frequently called the **bias or intercept term**. \n",
    "\n",
    "> _Note:_ <span style=\"color:gray\">\n",
    "At other points in the course, and by convention, we may represent the model using an equivalent expression written without the index $i$, namely:\n",
    "$$\\hat{y} = \\theta_0 + \\theta_1 x_{1} + \\dots + \\theta_p x_{p}$$\n",
    "> When written out like this, the symbols $x_{i, j}$ and $x_{j}$ are functionally identical - both refer to the $j^{th}$ feature of the $i^{th}$ datapoint. The $i$ is implicit in the case of the latter, but we are still talking in terms of numerical values, not vectors.\n",
    "</span>\n",
    "\n",
    "\n",
    "In order to construct the design matrix $\\mathbb{X} \\in \\mathbb{R}^{n\\times(p + 1)}$ from a given `DataFrame`, `X`, with a dimension of $n$ rows by $p$ columns, we need to augment our `DataFrame` with a column of ones.\n",
    "\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "Below, implement `add_intercept`, which creates a design matrix such that the first (left-most) column is all ones. The function has two lines: you are responsible for constructing the all-ones column `bias_feature` using the `np.ones` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.ones.html?highlight=ones)). This is then piped into a call to `np.concatenate` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html)), which we've implemented for you.\n",
    "\n",
    "**Note:** `bias_feature` should be a matrix of dimension `(n,1)`, not a vector of dimension `(n,)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_intercept(X):\n",
    "    \"\"\"\n",
    "    Return X with a bias feature.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: a 2D DataFrame of p numeric features\n",
    "    (may also be a 2D NumPy array) of shape n x p\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A 2D matrix of shape n x (p + 1), where the leftmost\n",
    "    column is a column vector of 1's.\n",
    "    \"\"\"\n",
    "    bias_feature = ...\n",
    "    return np.concatenate([bias_feature, X], axis=1)\n",
    "\n",
    "# Note the [[ ]] brackets below: the argument needs to be\n",
    "# a matrix (DataFrame), as opposed to a single array (Series).\n",
    "X = add_intercept(vehicle_data[['horsepower']])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 1b: Define the OLS Model\n",
    "\n",
    "The predictions for all $n$ points in our data are:\n",
    "$$ \\Large \\hat{\\mathbb{Y}} = \\mathbb{X}\\theta $$\n",
    "where $\\theta = [\\theta_0, \\theta_1, \\dots, \\theta_p]$.\n",
    "\n",
    "Below, implement the `linear_model` function to evaluate this product.\n",
    "\n",
    "**Hint**: You can use `np.dot` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)), `pd.DataFrame.dot` ([documentation](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dot.html)), or the `@` operator to multiply matrices/vectors. However, while the `@` operator can be used to multiply `NumPy` arrays, it generally will not work between two `pandas` objects, so keep that in mind when computing matrix-vector products!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q1b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linear_model(thetas, X):\n",
    "    \"\"\"\n",
    "    Return the linear combination of thetas and features as defined in the OLS equation.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    thetas: a 1D vector representing the parameters of our model ([theta0, theta1, ...]).\n",
    "    X: a 2D DataFrame of numeric features (may also be a 2D NumPy array).\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A 1D vector representing the linear combination of thetas and features as defined in the OLS equation.\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1c: Least Squares Estimate, Analytically\n",
    "\n",
    "Recall from lecture that Ordinary Least Squares is when we fit a linear model using Mean Squared Error (MSE), which is equivalent to the following optimization problem:\n",
    "\n",
    "$$\\Large \\min_{\\theta} ||\\Bbb{X}\\theta - \\Bbb{Y}||^2$$\n",
    "\n",
    "We showed in lecture that when $X^TX$ is invertible, the optimal estimate, $\\hat{\\theta}$, is given by the equation:\n",
    "\n",
    "$$ \\Large \\hat{\\theta} = (\\Bbb{X}^T\\Bbb{X})^{-1}\\Bbb{X}^T\\Bbb{Y}$$\n",
    "\n",
    "Below, implement the analytic solution to $\\hat{\\theta}$ using `np.linalg.inv` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html)) to compute the inverse of $\\Bbb{X}^T\\Bbb{X}$.\n",
    "\n",
    "**Hint 1**: To compute the transpose of a matrix, you can use `X.T` or `X.transpose()` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.T.html#numpy.ndarray.T)).\n",
    "\n",
    "**Note:** You can also consider using `np.linalg.solve` ([documentation](https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html)) instead of `np.linalg.inv` because it is more robust (more on StackOverflow [here](https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li)). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_analytical_sol(X, y):\n",
    "    \"\"\"\n",
    "    Computes the analytical solution to our\n",
    "    least squares problem\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    X: a 2D DataFrame (or NumPy array) of numeric features.\n",
    "    y: a 1D vector of outputs.\n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    The estimate for theta (a 1D vector) computed using the\n",
    "    equation mentioned above.\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "Y = vehicle_data['mpg']\n",
    "analytical_thetas = get_analytical_sol(X, Y)\n",
    "analytical_thetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Now, let's analyze our model's performance. Your task will be to interpret the model's performance using the two visualizations and one performance metric we've implemented below.\n",
    "\n",
    "First, we run `sns.lmplot`, which will both provide a scatterplot of `mpg` vs `horsepower` and display the least-squares line of best fit. (If you'd like to verify the OLS fit you found above is the same line found through `Seaborn`, change `include_OLS` to `True`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_OLS = False # Change this flag to visualize OLS fit\n",
    "\n",
    "sns.lmplot(data=vehicle_data, x='horsepower', y='mpg');\n",
    "predicted_mpg_hp_only = linear_model(analytical_thetas, X)\n",
    "if include_OLS:\n",
    "    # if flag is on, add OLS fit as a dotted red line\n",
    "    plt.plot(vehicle_data['horsepower'], predicted_mpg_hp_only, 'r--')\n",
    "plt.title(\"mpg vs horsepower\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we **plot the residuals.** While in Simple Linear Regression we have the option to plot residuals vs. the single input feature, in Multiple Linear Regression we often plot residuals vs. fitted values $\\hat{\\mathbb{Y}}$. In this lab, we opt for the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(predicted_mpg_hp_only, Y - predicted_mpg_hp_only)\n",
    "plt.axhline(0, c='black', linewidth=1)\n",
    "plt.xlabel(r'Fitted Values $\\hat{\\mathbb{Y}}$')\n",
    "plt.ylabel(r'Residuals $\\mathbb{Y} - \\hat{\\mathbb{Y}}$');\n",
    "plt.title(\"Residual plot\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally, we compute the **correlation r** and **Multiple $R^2$** metric. As described in Lecture 12,\n",
    "\n",
    "$$R^2 = \\frac{\\text{variance of fitted values}}{\\text{variance of true } y} = \\frac{\\sigma_{\\hat{y}}^2}{\\sigma_y^2}$$\n",
    "\n",
    "$R^2$  can be used\n",
    "in the multiple regression setting, whereas $r$ (the correlation coefficient) is restricted to SLR since it depends on a single input feature.  In SLR, $r^{2}$ and Multiple $R^{2}$ are\n",
    "equivalent; the proof is left to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_hp_only = np.corrcoef(X[:, 1], Y)[0, 1]\n",
    "r2_hp_only = r_hp_only ** 2\n",
    "R2_hp_only = np.var(predicted_mpg_hp_only) / np.var(Y)\n",
    "\n",
    "print('Correlation, r, using only horsepower: ', r_hp_only)\n",
    "print('Correlation squared, r^2, using only horsepower: ', r2_hp_only)\n",
    "print('Multiple R^2 using only horsepower: ', r2_hp_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 1d\n",
    "\n",
    "In the cell below, comment on the above visualization and performance metrics, and whether `horsepower` and `mpg` have a good linear fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: Transform a Single Feature\n",
    "\n",
    "The Tukey-Mosteller Bulge Diagram (shown below) tells us to transform our $\\mathbb{X}$ or $\\mathbb{Y}$ to find a linear fit.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"tukey_mosteller.png\" width=\"300vw\" /></div>\n",
    "\n",
    "Let's consider the following linear model:\n",
    "\n",
    "$$\\text{predicted mpg} = \\theta_0 + \\theta_1 \\sqrt{\\text{horsepower}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "### Question 2a\n",
    "\n",
    "In the cell below, explain why we use the term \"linear\" to describe the model above, even though it incorporates a square root of horsepower  as a feature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to `sklearn`\n",
    "\n",
    "Another way to fit a linear regression model is to use `scikit-learn`, an industry-standard package for machine learning applications. Because it is application-specific, `sklearn` is often faster and more robust than the analytical or `scipy`-based computation methods we've used thus far. Note that `scikit-learn` and `sklearn` refers to the same package, but it can only be imported under the name `sklearn`. We will use these two names interchangeably in this class.\n",
    "\n",
    "To use `sklearn`:\n",
    "\n",
    "1. Create an `sklearn` object.\n",
    "1. `fit` the object to data.\n",
    "1. Analyze fit or call `predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **1. Create object.** \n",
    "\n",
    "We first create a `LinearRegression` object. Here's the `sklearn` [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html). Note that by default, the object will include an intercept term when fitting.\n",
    "\n",
    "Here, `model` is like a \"blank slate\" for a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Run this cell to initialize a sklearn LinearRegression object.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# the `fit_intercept` argument controls whether or not the model should have an intercept (or bias) term\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **2. `fit` the object to data.** \n",
    "\n",
    "Now, we need to tell `model` to \"fit\" itself to the data. Essentially, this is doing exactly what you did in the previous part of this lab (creating a risk function and finding the parameters that minimize that risk).\n",
    "\n",
    "**Note**: `X` needs to be a matrix (or `DataFrame`), as opposed to a single array (or `Series`) when running `model.fit`. This is because `sklearn.linear_model` is robust enough to be used for multiple regression, which we will look at later in this lab. This is why we use the double square brackets around `sqrt(hp)` when passing in the argument for `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Run this cell to add sqrt(hp) column for each car in the dataset.\n",
    "vehicle_data['sqrt(hp)'] = np.sqrt(vehicle_data['horsepower'])\n",
    "vehicle_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2. Run this cell to fit the model to the data.\n",
    "model.fit(X = vehicle_data[['sqrt(hp)']], y = vehicle_data['mpg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3. Analyze fit.** \n",
    "\n",
    "Now that the model exists, we can look at the $\\hat{\\theta}_0$ and $\\hat{\\theta}_1$ values it found, which are given in the attributes `intercept` and `coef`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the `sklearn` linear regression model to make predictions, you can use the `model.predict` method.\n",
    "\n",
    "Below, we find the estimated `mpg` for a single datapoint with a `sqrt(hp)` of 6.78 (i.e., horsepower 46). Unlike the linear algebra approach, we do not need to manually add an intercept term because our `model` (which was created with `fit_intercept=True`) will automatically add one.\n",
    "\n",
    "**Note:** You may receive a user warning about missing feature names. This is due to the fact that we fitted on the feature DataFrame `vehicle_data[['sqrt(hp)']]` with feature names `\"sqrt(hp)\"` but only pass in a simple 2D arrays for prediction. To avoid this, we can convert our 2D array into a DataFrame with the matching feature name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needs to be a 2D array since the X in step 2 was 2-dimensional.\n",
    "single_datapoint = [[6.78]]\n",
    "# Uncomment the following to see the result of predicting on a DataFrame instead of 2D array.\n",
    "#single_datapoint = pd.DataFrame([[6.78]], columns = ['sqrt(hp)']) # \n",
    "model.predict(single_datapoint) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b\n",
    "\n",
    "Using the model defined above, which takes in `sqrt(hp)` as an input explanatory variable, predict the `mpg` for the full `vehicle_data` dataset. Assign the predictions to `predicted_mpg_hp_sqrt`. Running the cell will then compute the multiple $R^2$ value and create a linear regression plot for this new square root feature, overlaid on the original least squares estimate (used in Question 1c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_mpg_hp_sqrt = ...\n",
    "\n",
    "# Do not modify below this line.\n",
    "r2_hp_sqrt = np.var(predicted_mpg_hp_sqrt) / np.var(vehicle_data['mpg'])\n",
    "print('Multiple R^2 using sqrt(hp): ', r2_hp_sqrt)\n",
    "\n",
    "sns.lmplot(x = 'horsepower', y = 'mpg', data = vehicle_data)\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_hp_sqrt,\n",
    "         color = 'r', linestyle='--', label='sqrt(hp) fit');\n",
    "plt.title(\"mpg vs. horsepower\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization shows a slight improvement, but the points on the scatter plot are still more \"curved\" than our prediction line. Let's try a quadratic feature instead! \n",
    "\n",
    "Next, we use the power of OLS to **add an additional feature.** Questions 1 and 2 utilized simple linear regression, a special case of OLS where we have 1 feature ($p=1$). For the following questions, we'll utilize multiple linear regression, which are cases of OLS when we have more than 1 features ($p > 1$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Add an Additional Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part of this lab, we move from SLR to multiple linear regression.\n",
    "\n",
    "Until now, we have established relationships between one independent explanatory variable and one response variable. However, with real-world problems, you will often want to use **multiple features** to model and predict a response variable. Multiple linear regression attempts to model the relationship between two or more explanatory variables and a response variable by fitting a linear equation to the observed data.\n",
    "\n",
    "We can consider including functions of existing features as **new features** to help improve the predictive power of our model. (This is something we will discuss in further detail in the Feature Engineering lecture.)\n",
    "\n",
    "The cell below adds a column that contains the square of the horsepower for each car in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to add a column of horsepower squared, no further action needed.\n",
    "vehicle_data['hp^2'] = vehicle_data['horsepower'] ** 2\n",
    "vehicle_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "## Question 3\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "Using `sklearn`'s `LinearRegression`, create and fit a model that tries to predict `mpg` from `horsepower` AND `hp^2` using the DataFrame `vehicle_data`. Name your model `model_multi`.\n",
    "\n",
    "**Hint**: It should follow a similar format as Question 2.\n",
    "\n",
    "**Note**: You must create a new model again using `LinearRegression()`, otherwise the old model from Question 2 will be overwritten. If you do overwrite it, don't fret! Just restart your kernel and run your cells in order. If you are unsure why this overwritting happens, please review [object-oriented programming](https://cs61a.org/study-guide/objects/) from CS61A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_multi = LinearRegression() # By default, fit_intercept=True\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After fitting, we can see the coefficients and intercept. Note that there are now two elements in `model_multi.coef_`, since there are two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "Using the above values, write out the function that the model is using to predict `mpg` from `horsepower` and `hp^2`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "The plot below shows the prediction of our model. It's much better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to show the prediction of our model.\n",
    "predicted_mpg_multi = model_multi.predict(vehicle_data[['horsepower', 'hp^2']])\n",
    "r2_multi = np.var(predicted_mpg_multi) / np.var(vehicle_data['mpg'])\n",
    "print('Multiple R^2 using both horsepower and horsepower squared: ', r2_multi)\n",
    "\n",
    "sns.scatterplot(x = 'horsepower', y = 'mpg', data = vehicle_data)\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_hp_only, label='hp only');\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_hp_sqrt, color = 'r', linestyle='--', label='sqrt(hp) fit');\n",
    "plt.plot(vehicle_data['horsepower'],  predicted_mpg_multi, color = 'gold', linewidth=2, label='hp and hp^2');\n",
    "plt.title(\"mpg vs horsepower\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3c\n",
    "\n",
    "In the cell below, we assign the mean of the `mpg` column of the `vehicle_data` DataFrame to `mean_mpg`. Given this information, what is the mean of the `mean_predicted_mpg_hp_only`, `predicted_mpg_hp_sqrt`, and `predicted_mpg_multi` arrays?\n",
    "\n",
    "**Hint**: Your answer should be a function of `mean_mpg` provided, you should not have to call `np.mean` in your code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean_mpg = np.mean(vehicle_data['mpg'])\n",
    "mean_predicted_mpg_hp_only = ...\n",
    "mean_predicted_mpg_hp_sqrt = ...\n",
    "mean_predicted_mpg_multi = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing this model with previous models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compares q1, q2, q3, and overfit models (ignores redundant model)\n",
    "print('Multiple R^2 using only horsepower: ', r2_hp_only)\n",
    "print('Multiple R^2 using sqrt(hp): ', r2_hp_sqrt)\n",
    "print('Multiple R^2 using both hp and hp^2: ', r2_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the R^2 value of the last model is the highest. In fact, it can be proven that multiple R^2 will not decrease as we add more more variables. You may be wondering, what will happen if we add more variables? We will discuss the limitations of adding too many variables in an upcoming lecture. Below, we consider an extreme case that we include a variable twice in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Faulty Feature Engineering: Redundant Features\n",
    "\n",
    "Suppose we used the following linear model:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{mpg} &= \\theta_0 + \\theta_1 \\cdot \\text{horsepower} + \\theta_2 \\cdot \\text{horsepower}^2 + \\theta_3 \\cdot \\text{horsepower}\n",
    "\\end{align}\n",
    "\n",
    "Notice that `horsepower` appears twice in our model!! We will explore how this redundant feature affects our modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 4\n",
    "\n",
    "### Question 4a: Linear Algebra\n",
    "\n",
    "Construct a matrix `X_redundant` that uses the `vehicle_data` DataFrame to encode the \"three\" features above, as well as a bias feature.\n",
    "\n",
    "**Hint**: Use the `add_intercept` term you implemented in Question 1a.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_redundant = ...\n",
    "X_redundant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Now, run the cell below to find the analytical OLS Estimate using the `get_analytical_sol` function you wrote in Question 1c.\n",
    "\n",
    "**Note:** Depending on the machine that you run your code on, you should either **see a singular matrix error** or **end up with thetas that are nonsensical** (magnitudes greater than $10^{15}$). In other words, if the cell below errors, that is by design, it is supposed to error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to check the result, no further action needed. \n",
    "# The try-except block suppresses errors during submission\n",
    "import traceback\n",
    "try:\n",
    "    analytical_thetas = get_analytical_sol(X_redundant, vehicle_data['mpg'])\n",
    "    analytical_thetas\n",
    "except Exception as e:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "### Question 4b\n",
    "\n",
    "In the cell below, explain why we got the error above when trying to calculate the analytical solution to predict `mpg`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: While we encountered errors when using the linear algebra approach, a model fitted with `sklearn` will not encounter matrix singularity errors since it uses numerical methods to find optimums (to be covered in the Gradient Descent lecture)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn finds optimal parameters despite redundant features\n",
    "model_redundant = LinearRegression(fit_intercept=False) # X_redundant already has an intercept column\n",
    "model_redundant.fit(X = X_redundant, y = vehicle_data['mpg'])\n",
    "model_redundant.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nori congratulates you for finishing Lab 6!\n",
    "\n",
    "<div align=\"middle\">\n",
    "    <table style=\"width:100%\">\n",
    "      <tr align=\"center\">\n",
    "        <td><img src=\"nori1.jpeg\" align=\"middle\" width=\"250vw\" />\n",
    "        </td>\n",
    "        <td><img src=\"nori2.jpeg\" align=\"middle\" width=\"250vw\" />\n",
    "        </td>\n",
    "      </tr>\n",
    "    </table>\n",
    "  </div>\n",
    "\n",
    "## Submission\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. Please save before exporting and submit to the correct assignment on Gradescope (**Lab 06**)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1a": {
     "name": "q1a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X.shape == (392,2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (add_intercept(np.array([[1, 2, 3],[4, 5, 6]]).T)[:,0] == np.ones((3,))).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> add_intercept(np.array([[1, 2, 3],[4, 5, 6]]).T).shape == (3,3)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (add_intercept(np.array([[1, 2, 3],[4, 5, 6]]).T)[:,2] == np.array([4,5,6])).all()\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> linear_model(np.arange(1,5), np.arange(1,5)) == 30\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> (linear_model(2*np.eye(100), np.ones(100)) == 2*np.ones(100)).all()\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_theta = np.array([[1, 2], [3, 4], [5, 6]])\n>>> test_x = np.array([[1, 3, 5], [2, 4, 6]])\n>>> expected = np.array([[35, 44], [44, 56]])\n>>> actual = linear_model(test_theta, test_x)\n>>> np.array_equal(actual, expected)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> test_theta_2 = np.array([[3], [5]])\n>>> test_x_2 = np.array([[1, 4], [1, 6], [1, 8]])\n>>> expected_2 = np.array([[23], [33], [43]])\n>>> actual_2 = linear_model(test_theta_2, test_x_2)\n>>> np.array_equal(expected_2, actual_2)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> analytical_thetas.shape in ((2,), (2, 1))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(analytical_thetas[0], 39.93586102)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(model_multi.intercept_, 56.900099702112954)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(model_multi.coef_[0], -0.46618963)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(model_multi.coef_[1], 0.00123054)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> # Make sure our old model is not overwritten.\n>>> np.isclose(model.intercept_, 58.70517203721748) and np.isclose(model.coef_[0], -3.50352375)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3c": {
     "name": "q3c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> np.isclose(mean_predicted_mpg_hp_only, np.mean(predicted_mpg_hp_only), atol=0.001)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mean_predicted_mpg_hp_sqrt, np.mean(predicted_mpg_hp_sqrt), atol=0.001)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.isclose(mean_predicted_mpg_multi, np.mean(predicted_mpg_multi), atol=0.001)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> X_redundant.shape == (392,4)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
